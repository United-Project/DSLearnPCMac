{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92d899cf",
   "metadata": {},
   "source": [
    "# 3: Linear Regression\n",
    "\n",
    "## 3.1 Simple Linear Regression\n",
    "\n",
    "We predict a numerical response Y on the basis of a single predictor variable X. We assume an approximately linear relationship between X and Y. We may read $\\approx$ as \"is approximately modeled as.\"\n",
    "\n",
    "Linear relationship mathematically:\n",
    "\n",
    "$$\n",
    "Y \\approx \\beta_0 + \\beta_1X\n",
    "$$\n",
    "\n",
    "Another way to describe this equation is that we are regressing Y on X (or Y onto X).\n",
    "\n",
    "**Example:** If we were trying to see the relationship between advertising and sales, we can regress sales onto TV: $sales \\approx \\beta_0 + \\beta_1 * TV$. Both beta values are unknown constants that represent the intercept and slope in terms of the linear model. They are our parameters. We use our training data to produce estimates of the two beta values. We use these values to predict future sales.\n",
    "\n",
    "### 3.1.2 Assessing Accuracy of Coefficient Estimates\n",
    "\n",
    "How do we estimate the parameters/coefficients $\\beta_0$ and $\\beta_1$? We want to find an intercept $\\hat{\\beta_0}$ and slope $\\hat{\\beta_1}$ so that the resulting linear function graphically is \"close\" to our data.\n",
    "\n",
    "How do we measure closeness? The most common approach is the least squares criterion.\n",
    "\n",
    "**What is the residual sum of squares?**\n",
    "If we have an observed response $Y_i$ and a predicted response $\\hat{Y_i}$, the difference between these values represents the $i^{th}$ residual value $e_i = y_i - \\hat{y_i}$.\n",
    "\n",
    "The residual sum of squares (RSS):\n",
    "\n",
    "$$\n",
    "RSS = e^2_1 + e^2_2 + ... + e^2_n\n",
    "$$\n",
    "\n",
    "Assessing the accuracy of the coefficient estimates:\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1X + \\epsilon\n",
    "$$\n",
    "\n",
    "$\\beta_0$ is our intercept (expected value of $Y$ when $X=0$), while $\\beta_1$ is our slope (average increase in Y associated with increases in X). This model also serves as the population regression line, which is the best linear approximation for the relationship between Y and X. Our estimates for intercept and slope define the least squares line.\n",
    "\n",
    "**Bias vs unbiased estimates**\n",
    "When collecting a mean from a set of observations (sample), we might overestimate or underestimate the true mean for the population compared to our mean value. If we continue to take means from a large number of samples, then find the average of these means, we usually find a mean value that is equal to the actual population mean. This same principle applies for estimating our slope and intercept for our linear model.\n",
    "\n",
    "How much is our sample mean $\\hat{\\mu}$ from the population mean $\\mu$?\n",
    "We compute the standard error of $\\hat{\\mu}$ written as $SE(\\mu)$ with the following formula:\n",
    "\n",
    "$$\n",
    "Var(\\hat{\\mu}) = SE(\\hat{\\mu})^2 = \\frac{\\sigma^2}{n}\n",
    "$$\n",
    "\n",
    "Where $\\sigma$ is the standard deviation of each of the realizations $y_i$. Overall, the standard error tells us how much our sample mean differs from our population mean. With more observations $\\rightarrow$ decrease in standard error.\n",
    "\n",
    "**Confidence interval**\n",
    "If we take repeated samples to construct a confidence interval for each sample, 95% of intervals will contain the true unknown value of the parameter such as $\\beta_0, \\beta_1$.\n",
    "\n",
    "**Hypothesis test**\n",
    "We can have a null hypothesis and the alternate hypothesis:\n",
    "Null hypothesis $H_0$: There is no relationship between $X$ and $Y$.\n",
    "Alternate hypothesis $H_a$: There is some relationship between $X$ and $Y$.\n",
    "Mathematically, this means our slope is 0, indicating that there is no relationship between our X and Y, causing the formula to reduce to $Y = \\beta_0 + \\epsilon$.\n",
    "\n",
    "**The t-statistic**\n",
    "\n",
    "$$\n",
    "t = \\frac{\\hat{\\beta_1} - 0}{SE(\\hat{\\beta_1})}\n",
    "$$\n",
    "\n",
    "This measures the number of standard deviations that our slope $\\hat{\\beta_1}$ is away from 0. If there is no relationship between X and Y, then we expect a t-distribution with $n-2$ degrees of freedom. We use this statistic in order to quantify whether we should accept or reject the null hypothesis.\n",
    "\n",
    "**P-value**\n",
    "The probability of observing any number equal to the absolute value of our t-statistic. A small p-value indicates that it is likely to observe a true relationship between the predictor $X$ and the response $Y$; thus, we reject the null hypothesis declaring that there is a relationship (we accept the alternate hypothesis).\n",
    "\n",
    "### 3.1.3 Assessing the Accuracy of the Model\n",
    "\n",
    "We test the accuracy of a model (usually) with the residual standard error (RSE) and the $R^2$ statistic.\n",
    "\n",
    "**Residual Standard Error**\n",
    "**Def:** The RSE is an estimate of the standard deviation of the error term $\\epsilon$, meaning how much the predicted response $\\hat{y_i}$ will deviate from the true regression line (even when $\\beta_0, \\beta_1$ are known). This RSE value is a measurement of the lack of fit of the model to the data $\\rightarrow$ small RSE value implies the data fits very well.\n",
    "\n",
    "**$R^2$ Statistic**\n",
    "Measures the linear relationship between X and Y.\n",
    "Alternative measure of \"fit.\" It explains the proportion of variance. It is independent of our Y values.\n",
    "\n",
    "Total variance: Think about trying to guess a student's final exam score without any other information. You would probably try to find the natural spread or variability among all students. With this, you will probably guess the mean or average score.\n",
    "\n",
    "TSS $\\rightarrow$ Total Sum of Squares.\n",
    "Anaology: The TSS quantifies how much individual student scores deviae from this average. The total variability in Y we want to explain. \n",
    "TSS measures the total variance in the response Y, think of it as the inherent variability in the response before we apply regression.\n",
    "\n",
    "RSS $\\rightarrow$ Residual Sum of Squares\n",
    "Analogy: The residual sum of squares is our prediction of the test score after we have more information (an x-variable) such as hours studied.\n",
    "The *residual* value is the difference between the actual test score and your prediction. The Residual Sum of Squares (RSS) quantifies how much the actual scores still deviate from your model's predictions. It's the \"remaining mystery\" or \"unexplained variability\" after your model has done its best.\n",
    "\n",
    "TSS - RSS $\\rightarrow$ Total Variability - Unexplained Variability = Explained Variability\n",
    "Measures the amount of variability in the response that is explained (or removed) by performing the regression.\n",
    "\n",
    "$R^2$ Formula:\n",
    "$$\n",
    "R^2 = 1 - \\frac{RSS}{TSS} = \\frac{Explained \\; Variance}{Total \\; Variance}\n",
    "$$\n",
    "Measures the proportion of the variability in Y that can be explained using X. Greater values indicate a proportion of variability that is explained well by the regression, while low values indicate that the regression did not explain much of the variability in the response. (Variance is too high or the linear model is just a poor fit).\n",
    "Analogy: If TSS is the total pie, and RSS is your slice of \"hours studied\" that your model couldn't explain, then $R^2$ tells you what proportion of the total pie your model could explain.\n",
    "\n",
    "## 3.2 Multiple Linear Regression\n",
    "\n",
    "An extension of linear regression to accommodate for multiple predictors. This is done by applying multiple slope coefficients in a single model (equation) taking the form $Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p + \\epsilon$, where each $X_j$ is the $j^{th}$ predictor and $\\beta_j$ is the quantified association between that predictor and its $Y$ response (It is the average effect on Y of a one unit increase in $X_j$ if all other predictors were fixed). In other words, it is the effect on Y if we were to ONLY do a one unit increase on a specific $X_j$ and keep all other variables the same.\n",
    "\n",
    "### 3.2.1 Estimating the Regression Coefficients\n",
    "\n",
    "Use least squares approach to minimize the sum of squared residuals.\n",
    "\n",
    "Something to note: The coefficient estimates in, let's say, a model with 3 predictors will not always be equal to the coefficient estimates in 3 single variable linear models for the 3 predictors.\n",
    "\n",
    "### 3.2.2 Some Important Questions\n",
    "\n",
    "We are interested in answering the following questions when performing multiple linear regression:\n",
    "\n",
    "**Is at least one of the predictors $X_1, X_2, ..., X_p$ useful in predicting the response?**\n",
    "Are all of the regression coefficients zero?\n",
    "$\\beta_1 = \\beta_2 = ... = \\beta_p = 0$ being our hypothesis test. And our alternative hypothesis being if at least one $\\beta_j$ being non-zero.\n",
    "This test is performed by computing the F-statistic.\n",
    "\n",
    "**Properties of F-Statistic:**\n",
    "No relationship $\\rightarrow$ expect F-statistic close to 1. Relationship $\\rightarrow$ F is greater than 1.\n",
    "Adjust for the number of predictors, allowing for only a 5% chance that we have a p-value below 0.05 instead of having 100 individual p-values with 5 of them being able to have a p-value below 0.05 by chance.\n",
    "Cannot be used when our number of predictors is greater than the number of observations.\n",
    "\n",
    "**Do all the predictors help to explain Y, or is only a subset of the predictors useful?**\n",
    "In order to determine which subset of predictors are truly associated with our response, we turn to variable selection. (Studied more extensively in chapter 6)\n",
    "\n",
    "**Brief overview of variable selection:**\n",
    "* **Forward Selection:** Begin with a null model (model with an intercept but no predictors) and fit p linear regressions, find the variable that results in the lowest RSS value and then add this regression to the null model. Continue doing so until you would like to stop adding predictors to your model (null $\\rightarrow$ linear regression $\\rightarrow$ two variable model $\\rightarrow$ so on).\n",
    "* **Backward Selection:** Start with all variables in the model and remove variables with the largest p-value (as they are the least statistically significant). Every $(p-1)$-variable is more fit than the previous p variable.\n",
    "* **Mixed Selection:** Start with the null model, add variables 1-by-1 with the lowest p-value. As we know, in multiple linear regression, as more predictors are in a model, this can affect other predictors' p-values, so continue adding variables only until a certain threshold is met (we added a p-value that initially seemed small (in a single linear regression), but in our multiple linear regression, it is above this threshold).\n",
    "\n",
    "**How well does the model fit the data?**\n",
    "\n",
    "**R² (R-squared)**\n",
    "* **Definition:** R² measures the fraction of variance in the response variable (Y) explained by the predictor variables in the model.\n",
    "* **Formula:** $R^2 = 1 - \\frac{RSS}{TSS}$, where:\n",
    "    * $RSS$ = Residual Sum of Squares (variance unexplained by the model).\n",
    "    * $TSS$ = Total Sum of Squares (total variance in the response variable).\n",
    "* **Interpretation:** R² ranges from 0 to 1. A value close to 1 indicates the model explains a large portion of the variance in the response. In multiple regression, $R^2 = Correlation(Y, \\hat{Y})^2$.\n",
    "* **Key Points:** R² always increases when more predictors are added, even if they are weakly associated with the response. This can lead to overfitting. A small increase in R² when adding a predictor (e.g., newspaper advertising in the example) suggests the predictor may not improve the model significantly.\n",
    "\n",
    "**RSE (Residual Standard Error)**\n",
    "* **Definition:** RSE estimates the standard deviation of the residuals (prediction errors). It measures the average deviation of observed values from the predicted values.\n",
    "* **Formula:** $RSE = \\sqrt{\\frac{1}{n-p-1}RSS}$\n",
    "    * n = number of observations.\n",
    "    * p = number of predictors.\n",
    "* **Interpretation:** Lower RSE indicates a better fit, as the model's predictions are closer to the observed data. RSE can increase when adding a predictor if the decrease in RSS is small relative to the increase in p.\n",
    "* **Key Points:** RSE is more reliable than R² for comparing models with different numbers of predictors, as it penalizes unnecessary complexity.\n",
    "\n",
    "**Given a set of predictor values, what response value should we predict, and how accurate is our prediction?**\n",
    "We utilize prediction intervals to see how much $Y$ will vary from $\\hat{Y}$. These prediction intervals are always wider than confidence intervals as they incorporate both the reducible error in the estimate for $f(X)$ and uncertainty as to how much an individual point will differ from the population plane (irreducible error).\n",
    "\n",
    "## 3.3 Other Considerations in the Regression Model\n",
    "\n",
    "### 3.3.1 Qualitative Predictors\n",
    "\n",
    "**Prediction with only two levels:**\n",
    "The qualitative predictor, in this case, can only take a value of 1 or 0. For example: 1 if the $i^{th}$ person owns a house, and 0 if the $i^{th}$ person does not own a house.\n",
    "\n",
    "**More than two levels:**\n",
    "We must use more dummy variables. For example: $x_{i1}$ taking on 1 if $i^{th}$ person is from the south, and 0 if they are not. The second variable being $x_{i2}$ being 1 if $i^{th}$ person is from the west, and 0 if they are not.\n",
    "\n",
    "### 3.3.2 Extensions of the Linear Model\n",
    "\n",
    "The linear model has two assumptions (that suck):\n",
    "1.  The change (derivative) in which y changes based on x is a constant $\\rightarrow$ A one unit change in X results in a constant change in Y.\n",
    "2.  Additivity assumption $\\rightarrow$ association in a predictor and response does not depend on other predictors.\n",
    "\n",
    "**Removing the additive assumption:**\n",
    "This implies that predictors affect one another. Imagine the association is stronger between sales and TV ads compared to sales and radio ads. You would think we can put all of our budget into TV ads, but in reality, a combination of both is better. This is known as the **interaction effect** in statistics.\n",
    "\n",
    "We can extend the linear model by including a third predictor called the interaction term (product of predictors 1 and 2):\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_1X_2 + \\epsilon\n",
    "$$\n",
    "\n",
    "Our new term can be considered $\\hat{\\beta_1} = \\beta_1 + \\beta_3X_2$ or $\\hat{\\beta_2} = \\beta_2 + \\beta_3X_1$, meaning that a change in $X_2$ or $X_1$ changes the association between the predictor and Y. This is because $\\hat{\\beta}$ is now a function depending on X.\n",
    "\n",
    "Think of it as the change in effectiveness of TV advertising associated with a one-unit increase in radio advertising.\n",
    "\n",
    "**Hierarchical Principle**\n",
    "The main effects refer to individual contributions that each predictor variable has on the response variable (e.g., a single $X_1$ such as TV ads).\n",
    "The interaction effects are how the effect of one predictor on the response depends on the value of another predictor.\n",
    "This principle is a rule saying that if you include an interaction term $X_1X_2$, then you must include the individual, main effects ($X_1$ and $X_2$).\n",
    "\n",
    "### 3.3.3 Potential Problems\n",
    "Commonly occurring problems when fitting a linear regression.\n",
    "\n",
    "1.  **Non-linearity of the response-predictor relationships.**\n",
    "    We can identify non-linearity using **Residual Plots** (graphical tool). In simple linear regression we are plotting residuals $y_i - \\hat{y_i}$ versus the predictor $x_i$. In a multiple linear regression model, we instead plot residuals versus predictions (fitted) values $\\hat{y_i}$. If the plot **DOES** show a pattern, this indicates a potential problem with the linear model.\n",
    "\n",
    "2.  **Correlation of error terms.**\n",
    "    We assume the error terms to be uncorrelated. In essence, $\\epsilon_1$ being positive gives no information about $\\epsilon_2$. If the error terms are correlated, we may have an unwarranted sense of confidence in our model.\n",
    "\n",
    "3.  **Non-constant variance of error terms (Heteroscedasticity).**\n",
    "    Another assumption is that the error terms have a constant variance $Var(e_i)=\\sigma^2$. Standard errors, confidence intervals, and hypothesis tests rely on this assumption. One example of this would be a greater increase in the variance of residuals when the responses increase in value.\n",
    "\n",
    "4.  **Outliers.**\n",
    "    These are values very far away from the value predicted by the model. They can sometimes be due to data errors rather than a fault of the regression itself. Residual plots can be used to identify outliers. We use **studentized residuals**, computed by dividing each residual $e_i$ by its estimated standard error. On a residual plot, a studentized residual with a value greater than 3 is usually considered an outlier.\n",
    "\n",
    "5.  **High-leverage points.**\n",
    "    These are unusual values for our predictor $x_i$, distinct from an outlier in the response $y_i$. Essentially, it's a data point with an unusual predictor value that significantly affects our predictions. To quantify an observation's leverage, we compute the **leverage statistic**. Large values are associated with high leverage. For simple linear regression:\n",
    "    $$\n",
    "    h_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{i'=1}^{n}(x_{i'} - \\bar{x})^2}\n",
    "    $$\n",
    "\n",
    "6.  **Collinearity.**\n",
    "    This is the situation where two or more predictor variables are closely related to one another. It makes it difficult to single out the individual effects of collinear variables on the response.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.4 The Marketing Plan\n",
    "We will now return to the 7 questions about **Advertising** data that we set out to answer at the beginning of the chapter.\n",
    "\n",
    "1.  **Is there a relationship between sales and advertising budget?**\n",
    "    We would fit a multiple regression model of sales onto TV, radio, and newspaper and test the hypothesis $H_0: \\beta_{TV} = \\beta_{radio} = \\beta_{newspaper} = 0$ using the **F-statistic** to determine whether or not we reject the null hypothesis.\n",
    "\n",
    "2.  **How strong is the relationship?**\n",
    "    Use the **Residual Standard Error (RSE)** to estimate the standard deviation of the response from the population regression line. Secondly, use the **$R^2$ statistic** to record the percentage of variability in the response that is explained by the predictors.\n",
    "\n",
    "3.  **Which media are associated with sales?**\n",
    "    Use the **p-values** associated with each predictor's t-statistic.\n",
    "\n",
    "4.  **How large is the association between each medium and sales?**\n",
    "    The standard error $\\hat{\\beta_j}$ can be used to construct **confidence intervals** for $\\beta_j$. We would find the 95% confidence intervals for the coefficients in a multiple regression model using all three media budgets as predictors. Collinearity could be the reason a confidence interval for a variable (like newspaper) is so wide. Test the **VIF score**.\n",
    "\n",
    "5.  **Is the relationship linear?**\n",
    "    We will use **residual plots** to identify non-linearity. If the plots display a pattern, then the relationship is non-linear. We can introduce **transformations** (e.g., square root, log) to accommodate non-linear relationships.\n",
    "\n",
    "6.  **Is there synergy among the advertising media?**\n",
    "    We use an **interaction term** (multiplying two variables together) in order to accommodate non-additive relationships. A small p-value associated with the interaction term indicates the presence of such relationships.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.5 Comparison of Linear Regression with *K*-Nearest Neighbors\n",
    "As we know, linear regression is a **parametric method** making strong assumptions about the form of $f(X)$. Now we will look into **KNN regression**, which is a **non-parametric method**.\n",
    "\n",
    "Given a value for K and a prediction point $x_0$, KNN regression first identifies the K training observations that are closest to $x_0$, represented by $N_0$. It then estimates $f(x_0)$ using the average of all the training responses in $N_0$. In other words:\n",
    "$$\n",
    "\\hat{f}(x_0) = \\frac{1}{K} \\sum_{x_i \\in N_0} y_i.\n",
    "$$\n",
    "\n",
    "Finding an optimal K-value will depend on the **bias-variance tradeoff**. Larger values of K provide a smoother and less variable fit; the prediction in a region is an average of several points, so changing one observation has smaller effects. This smoothing, however, may cause bias and hide the true structure of $f(X)$.\n",
    "\n",
    "**When does a parametric approach (like least squares linear regression) outperform a non-parametric approach (like KNN regression)?** It outperforms when our assumption of the form of $f(X)$ is close to the true form of $f$.\n",
    "\n",
    "In **higher dimensions** (multiple predictors), KNN often performs worse than linear regression. The reason this occurs is that higher dimensions effectively cause a reduction in sample size. Spreading 50 observations over $p=20$ dimensions results in a phenomenon where a given observation has no *nearby neighbors*—this is the so-called **curse of dimensionality**. This happens when the nearest observations to a test observation are very far away in $p$-dimensional space when $p$ is large, leading to poor predictions.\n",
    "\n",
    "As a general rule, parametric methods outperform non-parametric approaches when there is a small number of observations per predictor. Sometimes it is worth it to have a slightly less accurate model that is more interpretable compared to a slightly more accurate, less interpretable model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4c0f35",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
